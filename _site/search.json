[
  {
    "objectID": "ENVX2001-2024-Lab07.html",
    "href": "ENVX2001-2024-Lab07.html",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "",
    "text": "Tip\n\n\n\nPlease work on this exercise by creating your own R Markdown file."
  },
  {
    "objectID": "ENVX2001-2024-Lab07.html#exercise-1-bird-abundance",
    "href": "ENVX2001-2024-Lab07.html#exercise-1-bird-abundance",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Exercise 1: Bird abundance",
    "text": "Exercise 1: Bird abundance\nThis is the same dataset used in the lecture.\nFragmentation of forest habitat has an impact of wildlife abundance. This study looked at the relationship between bird abundance (bird ha-1) and the characteristics of forest patches at 56 locations in SE Victoria.\nThe predictor variables are:\n\nALT Altitude (m)\nYR.ISOL Year when the patch was isolated (years)\nGRAZE Grazing (coded 1-5 which is light to heavy)\nAREA Patch area (ha)\nDIST Distance to nearest patch (km)\nLDIST Distance to largest patch (km)\n\nImport the data from the “Loyn” tab in the MS Excel file.\n\nlibrary(readxl)\nloyn &lt;- read_xlsx(\"mlr.xlsx\", \"Loyn\")\n\nOften, the first step in model development is to examine the data. This is a good way to get a feel for the data and to identify any issues that may need to be addressed. In this case, we will examine the data using histograms and a correlation matrix.\n\nHistograms\nThere are a breadth of ways to create histograms in R. In each tab below you will find some different ways to create the same plot outputs.\n\nhist()hist.data.frame() from Hmiscggplot()ggplot() with dplyr\n\n\nThis is a straightforward way to create multiple histograms with hist(). The par() function is used to arrange the plots on the page. The mfrow argument specifies the number of rows and columns of plots.\n\npar(mfrow=c(3,3))\nhist(loyn$ABUND)\nhist(loyn$ALT)\nhist(loyn$YR.ISOL)\nhist(loyn$GRAZE)\nhist(loyn$AREA)\nhist(loyn$DIST)\nhist(loyn$LDIST)\npar(mfrow=c(1,1))\n\n\n\n\n\n\nThe Hmisc package provides a function hist.data.frame() that can be used to create multiple histograms, which can be called by simply using hist(). You may need to tweak the nclass argument to get the desired number of bins, as the default may not look appropriate.\n\n# install.packages(\"Hmisc\")\nlibrary(Hmisc)\nhist(loyn, nclass = 20)\n\n\n\n\n\n\nA more modern approach is to use ggplot() with facet_wrap() to arrange multiple plots on a single page. To do this, the pivot_longer() function from the tidyr package is used to reshape the data into a tidy format.\n\n# tidy the data\nloyn_tidy &lt;- pivot_longer(loyn, cols = everything())\n\n# plot\nggplot(loyn_tidy, aes(x = value)) + \n  geom_histogram() + \n  facet_wrap(~name, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\nHere we use the pipe operator %&gt;% from dplyr to chain together a series of commands. The pipe operator takes the output of the command on the left and passes it to the command on the right (or below) the pipe. This means that we can create a series of commands that are executed in order.\n\nloyn %&gt;% \n  pivot_longer(cols = everything()) %&gt;% \n  ggplot(aes(x = value)) + \n  geom_histogram() + \n  facet_wrap(~name, scales = \"free\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 1\n\n\n\nComment on the histograms in terms of leverage. Hint: what is the relationship between leverage and skewness?\n\n\n\n\nCorrelation matrix\nCalculate the correlation matrix using cor(Loyn).\n\ncor(loyn)\n\n              ABUND         AREA      YR.ISOL       DIST       LDIST\nABUND    1.00000000  0.255970206  0.503357741  0.2361125  0.08715258\nAREA     0.25597021  1.000000000 -0.001494192  0.1083429  0.03458035\nYR.ISOL  0.50335774 -0.001494192  1.000000000  0.1132175 -0.08331686\nDIST     0.23611248  0.108342870  0.113217524  1.0000000  0.31717234\nLDIST    0.08715258  0.034580346 -0.083316857  0.3171723  1.00000000\nGRAZE   -0.68251138 -0.310402417 -0.635567104 -0.2558418 -0.02800944\nALT      0.38583617  0.387753885  0.232715406 -0.1101125 -0.30602220\n              GRAZE        ALT\nABUND   -0.68251138  0.3858362\nAREA    -0.31040242  0.3877539\nYR.ISOL -0.63556710  0.2327154\nDIST    -0.25584182 -0.1101125\nLDIST   -0.02800944 -0.3060222\nGRAZE    1.00000000 -0.4071671\nALT     -0.40716705  1.0000000\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nWhich independent variables are useful for predicting the dependent variable abundance? Is there evidence for multi-collinearity?\n\n\n\n\n\n\n\nPlotting correlation\nExamine correlations visually using pairs() or corrplot() from the corrplot package.\n\nScatterplot matrixCorrelation matrix\n\n\n\npairs(loyn)\n\n\n\n\n\n\n\nlibrary(corrplot)\ncorrplot(cor(loyn))\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nAre there any trends visible from the plots?\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe can also bring in variance inflation factors (VIF) to help us identify multi-collinearity, but that is done only after we have selected a model.\n\n\n\n\nTransformations\nThe AREA predictor has a small number of observations with very large values. Apply a log10 transformation and label the new variable Loyn$L10AREA.\n\nloyn$L10AREA &lt;- log10(loyn$AREA)\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nWhy are we transforming AREA?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nRe-run pairs(Loyn) and create a histogram using the transformed value of AREA, how do the plots look?\n\n\n\nhist(loyn$L10AREA)\npairs(loyn)\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nIn preparation for modelling, transform the remaining skewed variables, DIST and LDIST the same way you did for AREA and examine the histogram and pairs plots using these new variables.\n\n\nMake sure you end up with two new variables labelled loyn$L10DIST and loyn$L10LDIST."
  },
  {
    "objectID": "ENVX2001-2024-Lab07.html#exercise-2-modelling-bird-abundance",
    "href": "ENVX2001-2024-Lab07.html#exercise-2-modelling-bird-abundance",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "Exercise 2: Modelling bird abundance",
    "text": "Exercise 2: Modelling bird abundance\nWe will now use the transformed data in loyn for this exercise. If you have not already figured out how to perform the transformation, or if something is wrong, you may use the loyn tab in the mlr.xlsx MS Excel document. Alternatively, the code to convert the data is below.\n\n# reset the data import just in case it has been modified\nloyn &lt;- read_xlsx(\"mlr.xlsx\", \"Loyn\")\n# make transformations\n\nloyn &lt;- loyn %&gt;%\n  mutate(L10AREA = log10(AREA),\n    L10DIST = log10(DIST),\n    L10LDIST = log10(LDIST))\n\n# check\nglimpse(loyn)\n\nRows: 56\nColumns: 10\n$ ABUND    &lt;dbl&gt; 5.3, 2.0, 1.5, 17.1, 13.8, 14.1, 3.8, 2.2, 3.3, 3.0, 27.6, 1.…\n$ AREA     &lt;dbl&gt; 0.1, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2…\n$ YR.ISOL  &lt;dbl&gt; 1968, 1920, 1900, 1966, 1918, 1965, 1955, 1920, 1965, 1900, 1…\n$ DIST     &lt;dbl&gt; 39, 234, 104, 66, 246, 234, 467, 284, 156, 311, 66, 93, 39, 4…\n$ LDIST    &lt;dbl&gt; 39, 234, 311, 66, 246, 285, 467, 1829, 156, 571, 332, 93, 39,…\n$ GRAZE    &lt;dbl&gt; 2, 5, 5, 3, 5, 3, 5, 5, 4, 5, 3, 5, 2, 1, 5, 5, 3, 3, 3, 2, 2…\n$ ALT      &lt;dbl&gt; 160, 60, 140, 160, 140, 130, 90, 60, 130, 130, 210, 160, 210,…\n$ L10AREA  &lt;dbl&gt; -1.0000000, -0.3010300, -0.3010300, 0.0000000, 0.0000000, 0.0…\n$ L10DIST  &lt;dbl&gt; 1.591065, 2.369216, 2.017033, 1.819544, 2.390935, 2.369216, 2…\n$ L10LDIST &lt;dbl&gt; 1.591065, 2.369216, 2.492760, 1.819544, 2.390935, 2.454845, 2…\n\n\n\nBest single predictor?\n\n\n\n\n\n\nQuestion 1\n\n\n\nObtain the correlation between ABUND and all of the predictor variables using cor(). Based on these, what would you expect to be the best single predictor of ABUND?\n\n\n\ncor(loyn)\n\n\n\nAssumptions and interpretation\n\n\n\n\n\n\nQuestion 2\n\n\n\nUse multiple linear regression to see whether ABUND can be predicted from L10AREA and GRAZE. Are the assumptions met? Is there a significant relationship? Note: we are using these 2 predictors as they have the largest absolute correlations. Use lm() and specify the model as ABUND ~ L10AREA + GRAZE.\n\n\n\nlm.mod1 &lt;- lm(ABUND~GRAZE + L10AREA, data=loyn)\n\npar(mfrow=c(2,2))\nplot(lm.mod1)\npar(mfrow=c(1,1))\n\nsummary(lm.mod1)\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nHow good is the model based on the (i) r2 (ii) adjusted r2? Use summary().\n\n\n\nsummary(lm.mod1)$r.squared\nsummary(lm.mod1)$adj.r.squared  \n\n\n\n\n\n\n\nQuestion 4\n\n\n\nWhich variable(s) has the most significant effect(s)? (Refer specifically to the t probabilities in the table of predictors and their estimated parameters or coefficients in the output of summary()). Interpret the p-values in terms of dropping predictor variables.\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nRepeat the multiple regression, but this time include YRS.ISOL as a predictor variable (it has the 3rd largest absolute correlation). This will allow you to assess the effect of YRS.ISOL with the other variables taken into account.\n\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nCheck assumptions, do the residuals look ok? If you are happy with the assumptions, you can proceed to interpret the model output.\n\n\n\n\n\n\n\n\nQuestion 7\n\n\n\nCompare the r2 and adjusted r2 values with those you calculated for the 2 predictor model, Which is the better model? Why?\n\n\n\nsummary(lm.mod2)"
  },
  {
    "objectID": "ENVX2001-2024-Lab07.html#at-your-own-time-california-streamflow",
    "href": "ENVX2001-2024-Lab07.html#at-your-own-time-california-streamflow",
    "title": "ENVX2001 Lab 07 - Regression model development",
    "section": "At your own time: California streamflow",
    "text": "At your own time: California streamflow\n\n\n\n\n\n\nNote\n\n\n\nThis additional exercise can be done at your own time. Most of the code are provided. You will need to run the code and interpret the results.\n\n\nThe following dataset contains 43 years of annual precipitation measurements (in mm) taken at (originally) 6 sites in the Owens Valley in California. I have reduced this to three variables labelled L10APSAB (Lake Sabrina), L10OBPC (Big Pine Creek), L10OPRC (Rock Creek), and the dependent variable stream runoff volume (measured in ML/year) at a site near Bishop, California (labelled L10BSAAM). There is also a variable Year but you can ignore this.\nNote the variables have already been log-transformed to increase normality of the residuals in the regressions.\nStart with a full model and manually remove the variables one at a time, checking every time whether removal of a variable actually improves the model.\n\n# read in the data\ns.data &lt;- read_xlsx(\"mlr.xlsx\", \"California_streamflow\")\nnames(s.data)\n\n[1] \"L10APSAB\" \"L10OBPC\"  \"L10OPRC\"  \"L10BSAAM\"\n\n\n\ns.mod_full &lt;-lm(L10BSAAM~L10APSAB + L10OBPC + L10OPRC, data=s.data)\ns.mod_full &lt;-lm(L10BSAAM~., data=s.data) ## you can also use the . to indicate use all variables\nsummary(s.mod_full)\n\n\nCall:\nlm(formula = L10BSAAM ~ ., data = s.data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.09885 -0.03331  0.01025  0.03359  0.09495 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.25716    0.12360  26.352  &lt; 2e-16 ***\nL10APSAB     0.05631    0.03756   1.499  0.14185    \nL10OBPC      0.21085    0.06756   3.121  0.00339 ** \nL10OPRC      0.43838    0.08798   4.983 1.32e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.04861 on 39 degrees of freedom\nMultiple R-squared:  0.8817,    Adjusted R-squared:  0.8726 \nF-statistic: 96.88 on 3 and 39 DF,  p-value: &lt; 2.2e-16\n\n\n\nPartial F-Tests\nThe above analysis tells us that both L10OBPC & L10OPRC are significant, according to the t-test, in the model and L10APSAB is not? This involves performing Partial F-Tests as discussed in the lecture.\nThis can be done in R by using anova() on two model objects. To be able to compare the models and run the anova, you need to make objects of all the possible model combinations you want to compare.\n\ns.mod_reduced &lt;- lm(L10BSAAM ~ L10OPRC + L10OBPC, data=s.data)\nanova(s.mod_reduced, s.mod_full)\n\nThe last row gives the results of the partial F-test.\n\n\n\n\n\n\nQuestion 1\n\n\n\nShould we remove L10APSAB from the model?\n\n\n\n\n\n\n\n\nQuestion 2\n\n\n\nIs the p-value for the f-test the same as for the t-test?\n\n\n\n\n\n\n\n\nQuestion 3\n\n\n\nWrite out the hypotheses you are testing.\n\n\nPerform a Partial F-Test to work out if the removal of L10APSAB and L10OBPC improves upon the full model.\n\ns.mod_reduced2  &lt;- lm(L10BSAAM ~ L10APSAB + L10OBPC,data=s.data)\nanova(s.mod_reduced2, s.mod_full)\n\nAnalysis of Variance Table\n\nModel 1: L10BSAAM ~ L10APSAB + L10OBPC\nModel 2: L10BSAAM ~ L10APSAB + L10OBPC + L10OPRC\n  Res.Df      RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     40 0.150845                                 \n2     39 0.092166  1   0.05868 24.83 1.321e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nQuestion 4\n\n\n\nWhich variable should be added to the model containing L10OPRC?\n\n\n\n\n\n\n\n\nQuestion 5\n\n\n\nCould things be even simpler? Perform a partial F-Test to see if a model containing L10OPRC alone could be suitable.\n\n\n\ns.mod_reduced3  &lt;- lm(L10BSAAM ~ L10OPRC,data=s.data)\nanova(s.mod_reduced3, s.mod_full)\n\n\n\n\n\n\n\nQuestion 6\n\n\n\nWhat is your optimal model?\n\n\n\nThat’s it for today! Great work fitting simple and multiple linear regression! Next week we jump into stepwise selection and predictive modelling!"
  }
]